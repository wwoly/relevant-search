
%% ==========================================================
This chapter introduces the different implementations done to improve the relevance of the search results, and
the tests concluded to measure the effect that each implementation had.
The purpose of the implementations was to provide a general direction for future development 
while providing some robust data on the performance of the current search application for the organization.
The prerequisite for developing the methods was that if one were to succeed over the current
process, the organization would put more effort into developing that method further.



%% ==========================================================
\section{Implementing an improved search}


This section introduces how an improved search was implemented in this thesis.
First, the section provides an overview of the interpretation of how to achieve more relevant search 
results.
Second, the section introduces the methods that were used in creating tokens while indexing.
Lastly, the section describes how the popularity data was collected to replace
the manual process of uploading the score boost, before moving on to introduce how
the new ranking function was developed and how the score calculation implemented functioned.
Since relevance is subjective and personalized search was out of scope,
a general solution that tried to capture the general relevance of the search results
was implemented.




%% ==========================================================
\subsection{Creating tokens from content}
\label{ss:methodsTokens}


Here the process of generating tokens from the content of the products is described.
As previously mentioned, the token generation is a crucial part of a search engine, which indeed became
apparent during the development phase,
where a few different options were tested and considered, before finding the best one.


The development was done on the Finnish website, where the product texts are in Finnish.
Due to the nature of Finnish language having many inflected forms of words, 
it quickly became apparent that only the language analyzer was not sufficient.
The language analyzer could not interpret the meaning from the words and, 
while generating good enough tokens for most cases, failed to do so with longer words (Section \ref{ss:matchingTokens}).
The first thing where the development started was to investigate how more meaningful tokens could 
be generated.


A better Finnish analyzer was found, but it was not adopted because
it would have been required to be installed in the existing production Elasticsearch cluster, and, furthermore, 
the analyzer was only for Finnish. 
The other Nordic languages would have needed a different solution.
However, the lack of knowledge about the nuances of these languages made it impossible to determine
how well the language analyzers were performing in practice.


However, by looking at how the language analyzer worked at a lower level, it became clear that an edge-N-gram analyzer
could be utilized to achieve similar behavior as the language analyzer.
In addition to the similar behavior, the edge-N-gram could capture the meaning from a word better
when the number of generated N-grams could be controlled.
However, generating tokens with a minimum length of two was not enough to capture the meaning of the words, as 
quickly became apparent during development.


A similar problem arose during search time. 
If one of the N-grams matched a token in the inverted index, the document was considered a match.
While this is the expected and wanted result, it meant that when generating small N-grams, the precision
of the search results suffered greatly while the recall went up.
The low precision can be observed in Listing \ref{lst:puNGram}, by generating edge-N-grams with a minimum length of two from
the words ``puhelimet`` and ``punainen``.
\begin{lstlisting}[
caption={Output from the edge-N-gram analyzer with a minimum length of two.},
captionpos=b,
label={lst:puNGram},
frame=single]
    [ pu, puh, puhe, puhel, puheli, puhelim, ... ]
    [ pu, pun, puna, punai, punain, punaine, ... ]
\end{lstlisting}
As the first term translates to \emph{phones} and the second term to \emph{red}, the words mean entirely different things,
and neither should match the other one.


To solve these problems, a custom edge-N-gram analyzer was created (Figure \ref{fig:new-analyzer}).
The custom analyzer includes two different edge-N-gram analyzers. 
The first one, \emph{edge\_ngram\_analyzer}, is used during the indexing of the documents, 
and the second one, \emph{search\_term\_over\_6}, is used during search time.
Both analyzers utilize the same tokenizers and the first two token filters to keep the generated tokens consistent,
first by converting the words to lowercase and then converting them to ASCII characters.


The index time analyzer creates N-grams with a minimum token length of two and a maximum token length of 15 to try
to capture as much meaning from the words as possible.
The search time analyzer uses a conditional token filter, which determines if an N-gram should be created or not.
The search time tokenizer only generates N-grams if the search term is over six characters long.
The threshold of six letters was achieved by calculating the average length from the queried search terms over one month.
In addition, six letters seemed to capture the meaning from the words during development well enough that it was chosen.
In the future development, the process of choosing the threshold could be automated to find the optimal threshold
while balancing precision and recall of the search results.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/new-analyzer.png}
    \caption{Different analyzers for index time and search time.}
    \label{fig:new-analyzer}
\end{figure}

By using different analyzers during index time and search time allowed to create shorter N-grams to allow
matching with shorter search terms. 
In addition, the usage of an edge-N-gram analyzer in search time only when the search term exceeded the configured
threshold allowed the exact matching of the shorter search terms.
Furthermore, for longer search terms, the analyzer tried to capture the meaning of the terms and 
match the generated tokens to the inverted index.


%% ==========================================================
\subsection{Replacing the manual score boosts}


The manual process of updating the score boosts was replaced by an automated process to create a more general solution.
As the aim was not to create separate search strategies based on, for instance, the behavior of a customer, 
a precalculated boost value was enough.
Furthermore, the current solution used many parameters when calculating a new boost value for a product, and 
most of them seemed unnecessary and had little to do with relevance.

To answer the question, what customers see relevant, a popularity rating was introduced. 
The rating includes only a normalized value of the number of product page views from the previous few days.
Since the data was collected and stored by Google Analytics and was not available during indexing,
a new integration was required to import the data.
A new task was created to import the data every morning from the previous day and to saves the data
to persistent storage, where from the indexing process can fetch the data and calculate the value for the boost.


Calculating the boost based on the previous days may not be the most accurate method, at least in e-commerce, since 
many factors affect the popularity of a product from different trends to many campaigns and promotions.
To keep the rating as updated as possible, a custom function, shown in Figure \ref{fig:new-boost-rating}, was introduced.
The function receives an ordered list of the number of views and weight as input and 
calculates the decaying average starting from the first item on the list.
The function weighs the items at the start of the input less than the last items to allow the newer 
product page views to affect the resulting rating more than the older dates.


After calculating the decaying average, the function uses a natural logarithm to flatten the products with
a very high number of views.
A very high view count is not that meaningful since it may negatively affect the ranking,  
by overriding the content score altogether.
For instance, saying that a product with 10000 views is ten times more popular as a product with 1000 views might not be accurate.
The last step of the score calculation is to normalize the rating, given by the function, across all products
to a value between zero and one.
Normalization also prevents high business scores, which might cause the content score to lose meaning in 
the ranking functions.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/new-boost-rating.png}
    \caption{Function to calculate boost rating based on the number of views.}
    \label{fig:new-boost-rating}
\end{figure}


% The previous part introduced the new process introduced to replace the current manual process altogether.
% The process included an integration to import data about the number of product page views to the persistent
% storage and the number of views to be used to calculate a rating for the products in the index.
% The following part continues to introduce how the previous rating is utilized in the ranking algorithm,
% which determines the position of the products in the search results.


%% ==========================================================
\subsection{Ranking algorithm}
\label{ss:methodsRanking}

This section introduces how the new popularity rating was utilized in the ranking algorithm,
which determines the position of the products in the search results.
In total, there were two different ranking algorithms created for this thesis, both utilizing the
calculated popularity rating that was introduced in the previous section.
This section will introduce both algorithms and provide the reasoning for developing the first
one further.


The precalculated popularity rating serves as the base for the ranking algorithms.
The first iteration of the ranking algorithm only used the popularity rating 
as the business score.
After the first test concluded, it became clear that it did not perform as expected
since the conversion rate went down.
Since the first algorithm did not include anything else other than the popularity score as business scores,
the algorithm captured the intent of users well but did not meet the needs of the business.
The top products in search lists were relevant, but there were multiple cases that the product 
was not in stock.
A relevant search result was provided back to the user, but the search result was not buyable, 
which explained the decrease of the conversion rate.


The second algorithm created was similar to the first one with an additional boost if the product was in stock.
The second algorithm uses the following formula to calculate the score for a product
\[ score_{product} = 1 \times score_{popularity} + 0.3 \times score_{stock} + 1 \times score_{content} \]
where the popularity, stock, and content score are first multiplied with weight values, and then 
the sum of all the scores is calculated as the score for the product.
The $score_{stock}$ is a value of 1 or 0 depending on if the product has stock and customers can buy it.
The weight values for the individual parameters are a result of the empirical tests and should be a subject
for future development.


The value that changes the most in the ranking algorithms is the content score $score_{content}$.
The base content score is calculated in Elasticsearch by dividing the term frequency with the
inverse document frequency.
However, in this case, the first concluded test showed that the content score calculation in the first algorithm
could be improved since it did not capture exact matches well due to the wildcard nature of the query.

The second ranking algorithm, in addition to the updated business score, 
included multiple queries to boost the exact matching results better. 
An example of multiple queries is shown in Figure \ref{fig:new-query}.
Using multiple queries allowed the first query to use wildcard characters both at the start and at the end of 
the search term, the second query to utilize the wildcards at the end of the search term.
The last query did not use any wildcards to add a small additional boost to the exact matches.


Elasticsearch allows the usage of multiple queries at the same time within a \emph{should} clause.
If a product matches any of the three queries, it will be shown in the search results.
The only difference is if the product matches multiple queries, the content score
for the product will increase based on how well it matches to the query.
Each query has a \emph{boost} variable defined, which allows using different weights
for each query. 
In this setup, the exact queries did not receive a high boost value since it quickly increased
precision and decreased recall too much.
The selected boosts for each of the queries were the result of the empirical tests done during
development.


\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/new-query-v.png}
    \caption{Three parallel queries used across different fields.}
    \label{fig:new-query}
\end{figure}


Each of the queries defines a \emph{tie\_breaker} value, which tells Elasticsearch to search across all fields and
to utilize all scores in the content score calculation (Section \ref{sec:elasticsearch}).
Utilizing all fields instead of the best matching field reflected the content of the document better.
This approach seemed to provide better search results during the empirical tests, but in the end, using all
fields depends on the content in the fields.
Furthermore, if the content is very different in the fields, then the usage of only the \emph{best fields}
may be justified.


% The previous section introduced different methods for this thesis.
% Furthermore, the new methods proposed included new analyzers to generate meaningful tokens, 
% the new process to replace the manual process.
% In addition to a new ranking function, which utilized the calculate popularity rating for the product
% and used multiple queries to manipulate the content score calculation.
% The following sections continue to explain how the tests were concluded and how the test results 
% were analyzed before the results are introduced.


%% ==========================================================
\section{Testing the search relevance}
\label{sec:methodTests}

The tests were concluded with Google Optimize, which allows running different kinds of tests.
In this thesis, simple A/B tests were sufficient enough. 
Google Optimize requires a script to be loaded on the page, and it uses a cookie to determine 
which variant the customer belongs to.

Google Optimize offers a possibility to modify the part of a website while configuring the test, 
which allowed a custom tag to be added when a customer loads the page 
of a website and loads the Google Optimize script.
By injecting custom HTML tag to the webpage shown to the customer, the front-end JavaScript code can
determine which variant the customer belongs and set a custom cookie according to the variant.

When a customer interacted with the search input field and queried something, the receiving back-end
API determined via cookies which the variant of the customer and constructed the query differently
based on the variant.
After constructing the query and posting it through the Query API to the Elasticsearch search engine, 
the results were handled independently of the variant the customer belonged to.
Only by constructing the query differently based on a cookie value, different results could be shown 
to the customers, without any modifications to the code.


Only one of the different variants was used by a specific website along with the baseline variant.
Since the tests were concluded in three different websites across Nordic countries, and the codebase is the same
across the website back-ends, there was a need for a total of four variants.
Each variant had different functionality enabled.
For instance, \emph{Variant B} had only the new ranking algorithm,
which used the popularity rating and stock availability.
\emph{Variant C} had the new queries for calculating the
content score but used the old \emph{scoreBoost} field to calculate the business scores.
\emph{Variant D} had both enabled, and \emph{Variant A} was the baseline with nothing new features
enabled.
The specific test setups across the variants are introduced in detail in Section \ref{ch:results}.


% This section provided an overview of how the testing setup worked through the third party Google Optimize software.
% Furthermore, by creating a cookie to the browser of the customer, the back-end code could construct the Elasticsearch
% query differently based on the variant.
% The following section introduces how the results from the tests were analyzed before moving on to the actual results.




%% ==========================================================
\section{Analyzing the results}

Google Analytics is a powerful analytics tool that facilitates the collection and analysis of data for organizations.
In this study, Google Analytics collects the data and Google Optimize added variant identifiers to the data.
Both products are operated by Google and are used in many organizations across the world.


The metrics introduced in Section \ref{sec:measurements} were used as the primary measurements in this thesis.
For an in-depth analysis of the data, Google Analytics provides a variety of functions.
The measurements were collected based on the time frame of the tests and the identifier added by the Google Optimize.
A closer analysis of the results was not necessary to understand the effects of the concluded tests.
The collected measurements captured the behavior of the customers well, and the results reflect the
expectations moderately.
Although the data was collected across three different websites, the results were easy to differentiate since, 
for each website, there was an own Google Analytics project.

